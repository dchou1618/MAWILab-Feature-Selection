{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics:\n",
    "\n",
    "CPU \n",
    "Storage - A,B numbers are in storage\n",
    "Numbers read from storage into registers in cpu\n",
    "Then variables multiplied and stored in third register\n",
    "Third is written into storage\n",
    "\n",
    "If sequential:\n",
    "step latency - time it takes to finish 1 step\n",
    "total latency - time to finish all steps\n",
    "Main memory is fast\n",
    "spinning disk is slow\n",
    "\n",
    "Most latency is from memory latency, not operational latency\n",
    "Most latency is from reading and writing\n",
    "\n",
    "Sorting can improve memory locality\n",
    "Reason is money - local memory faster\n",
    "CPU checks if memory location in cache, and if it's not there,\n",
    "then retrieval will be slow. First case is a cache hit \n",
    "(when CPU intends to read a location and it's in cache)\n",
    "Second case is a cache miss.\n",
    "\n",
    "To handle a cache miss: \n",
    "(1) Free space in cache\n",
    "(2) After freeing, read in memory location and copy the block size\n",
    "Caches are effective if they have high hit rates.\n",
    "\n",
    "Unsorted word counts would entail temporal locality for common words, but\n",
    "no spatial locality.\n",
    "\n",
    "Caching reduces latency by bringing relevant data closer to the CPU.\n",
    "\n",
    "Access locality is the ability of software to make good use of the cache\n",
    "\n",
    "Temporal Locality = locality accessing the same elemnt over and over again\n",
    "\n",
    "Spatial Locality - ways to store n x terms to be squared. \n",
    "Linked lists have poor locality whereas indexed-arrays have\n",
    "good locality.\n",
    "\n",
    "Arrays store elements consecutively\n",
    "\n",
    "Row by row for array scanning is faster\n",
    "\n",
    "Effect increases proportionally with the number of elements in array\n",
    "\n",
    "\n",
    "Memory Hierarchy from small/fast storage closest to CPU at top.\n",
    "\n",
    "CPU (registers) - L2 Cache - Memory - Disk\n",
    "\n",
    "CPU registers -> L1/L2/L3 cache -> RAM -> SSDs -> Magnetic Tapes\n",
    "\n",
    "Data Centers are the physical aspect of \"the cloud\"\n",
    "\n",
    "\n",
    "HDFS - each file is broken into fixed-size chunks that are then copied.\n",
    "* Low cost per byte of storage \n",
    "* Locality\n",
    "* Redundancy - can recover from server failures\n",
    "* simple abstraction - looks like standard file system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .appName(\"portdata-causality\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.retainGroupColumns\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* \n",
    "ip_flags, tcp_flags_ack, ip_dsfield, tcp_seq, tcp_flags_fin, tcp_flags_urg,tcp_flags_push\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "probeDf = spark.read.csv(\"newPortData.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "df = probeDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "# \"ip_flags\", \"tcp_flags_ack\",\\\n",
    "#                 \"ip_dsfield\",\\\n",
    "#                 \"tcp_seq\", \"tcp_flags_fin\",\\\n",
    "#                 \"tcp_flags_urg\",\\\n",
    "#                 \"tcp_flags_push\",\\\n",
    "#                 \"tcp_options_mss_val\",\\\n",
    "#                 \"ip_ttl\",\\\n",
    "#                 \"tcp_window_size\",\\\n",
    "#                 \"tcp_checksum\",\\\n",
    "#                 \"tcp_srcport\",\\\n",
    "#                 \"tcp_dstport\",\\\n",
    "#                 \"label\"\n",
    "\n",
    "# \"frame_info_time\" is highly correlated with other variables\n",
    "# selected after feature importance with fake attributes\n",
    "probeDf = probeDf.select(*(\"frame_info_len\", \"tcp_ack\",\\\n",
    "                \"tcp_seq\",\\\n",
    "                \"ip_len\", \"tcp_flags\",\\\n",
    "                \"tcp_options_mss_val\",\\\n",
    "                \"ip_ttl\",\\\n",
    "                \"tcp_window_size\",\\\n",
    "                \"tcp_checksum\",\\\n",
    "                \"tcp_srcport\",\\\n",
    "                \"tcp_dstport\",\\\n",
    "                \"label\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "probeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.sql.functions import col\n",
    "for col_ in probeDf.columns[:len(probeDf.columns)-1]:\n",
    "    if col_ == \"tcp_options_mss_val\":\n",
    "        probeDf = probeDf.withColumn(col_, col(col_).cast(\"float\"))\n",
    "    else:\n",
    "        probeDf = probeDf.withColumn(col_, col(col_).cast(\"integer\"))\n",
    "probeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(probeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"frame_info_len\", \"tcp_ack\",\\\n",
    "                \"tcp_seq\",\\\n",
    "                \"ip_len\", \"tcp_flags\",\\\n",
    "                \"tcp_options_mss_val\",\\\n",
    "                \"ip_ttl\",\\\n",
    "                \"tcp_window_size\",\\\n",
    "                \"tcp_checksum\",\\\n",
    "                \"tcp_srcport\",\\\n",
    "                \"tcp_dstport\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.setHandleInvalid(\"skip\").transform(probeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "probeDf = output.select(*(\"features\",\"label\"))\n",
    "probeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\",\\\n",
    "    outputCol=\"indexedFeatures\",\\\n",
    "    maxCategories=3).fit(probeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "DF = probeDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "labelTypes = ['normal', 'nmap_null', 'nmap_connect', 'zmap', 'nmap_window', 'masscan', 'hping_syn', 'unicorn_null', 'unicorn_syn', 'nmap_xmas', 'nmap_syn', 'unicorn_conn', 'unicorn_xmas', 'nmap_ack', 'hping_fin', 'nmap_maimon', 'hping_null', 'hping_xmas', 'hping_ack', 'nmap_fin', 'unicorn_fxmas']\n",
    "labelMapping = {lab:i for i,lab in enumerate(labelTypes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"newPortData.csv\")\n",
    "\n",
    "indices = dict()\n",
    "for col in labelTypes:\n",
    "    indices[col] = list(df[df['label'] == col][\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "# doing data split\n",
    "\n",
    "newTest = None\n",
    "newTrain = None\n",
    "\n",
    "for col in labelTypes:\n",
    "    allCol = probeDf.filter(probeDf[\"label\"]==col)\n",
    "    newColFiltered = allCol.sample(False, 0.5, seed=101)\n",
    "    if newTest is None:\n",
    "        newTest = (allCol).subtract(newColFiltered)\n",
    "    else:\n",
    "        newTest = newTest.union((allCol).subtract(newColFiltered))\n",
    "    \n",
    "    if newTrain is None:\n",
    "        newTrain = newColFiltered\n",
    "    else:\n",
    "        newTrain = newTrain.union(newColFiltered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "#(trainingData, testData) = probeDf.randomSplit([0.70, 0.30])\n",
    "\n",
    "rfProbe = RandomForestClassifier(labelCol=\"indexedLabel\", \n",
    "                                 featuresCol=\"indexedFeatures\", \n",
    "                                 numTrees = 5,\n",
    "                                 maxDepth = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rfProbe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "model = pipeline.fit(newTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "predictions = model.transform(newTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "# looking to other models for classification\n",
    "\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "lstmDf = pd.read_csv(\"fullProbeData.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "lstmDf = lstmDf.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "lstmFeaturesDf = lstmDf[[\"frame_info_len\", \"tcp_ack\",\\\n",
    "                \"tcp_seq\",\"ip_len\", \"tcp_flags\",\\\n",
    "                \"tcp_options_mss_val\",\"ip_ttl\",\\\n",
    "                \"tcp_window_size\",\"tcp_checksum\",\\\n",
    "                \"tcp_srcport\",\"tcp_dstport\"]]\n",
    "lstmX = lstmFeaturesDf.values\n",
    "lstmY = lstmDf[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "lstmX, lstmY = oversample.fit_resample(lstmX, lstmY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from keras.utils import to_categorical\n",
    "encoded = to_categorical(lstmY-1)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from sklearn.model_selection import train_test_split\n",
    "lstmXTrain, lstmXTest, lstmYTrain, lstmYTest = \\\n",
    "    train_test_split(lstmX, encoded, test_size = 0.33, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "lstmXTrain = lstmXTrain.reshape((lstmXTrain.shape[0], lstmXTrain.shape[1], 1))\n",
    "lstmXTest = lstmXTest.reshape((lstmXTest.shape[0], lstmXTest.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.layers import Dropout, Input, Flatten,BatchNormalization\n",
    "from keras.models import Model\n",
    "def buildLSTMModel(input_shape):\n",
    "    lstmProbeModel = Sequential()\n",
    "    lstmProbeModel.add(LSTM(units=100,\n",
    "                           input_shape=input_shape))\n",
    "    lstmProbeModel.add(Dense(50, activation='relu'))\n",
    "    lstmProbeModel.add(Dropout(0.1))\n",
    "    lstmProbeModel.add(Dense(50, activation='tanh'))\n",
    "    lstmProbeModel.add(BatchNormalization())\n",
    "    lstmProbeModel.add(Dense(21, activation='softmax'))\n",
    "    lstmProbeModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(lstmProbeModel.summary())\n",
    "    lstmProbeModel.fit(lstmXTrain, lstmYTrain, validation_data=(lstmXTest, lstmYTest), epochs=5, batch_size=5000)\n",
    "    return lstmProbeModel\n",
    "lstmProbeModel = buildLSTMModel((lstmXTrain.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "from tensorflow.keras.models import save_model\n",
    "lstmProbeModel.save(\"lstmProbeModel.h5\")\n",
    "\n",
    "test_loss, test_acc = lstmProbeModel.evaluate(lstmXTest, lstmYTest)\n",
    "\n",
    "print(\"Test accuracy\", test_acc)\n",
    "print(\"Test loss\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "lstmXTrain2, lstmXTest2, lstmYTrain2, lstmYTest2 = \\\n",
    "    train_test_split(lstmX, encoded, test_size = 0.33, random_state=402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from tensorflow.keras.models import load_model\n",
    "lstmProbeModel2 = load_model(\"./lstmProbeModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "lstmProbeModel2.fit(lstmXTrain2, lstmYTrain2, validation_data=(lstmXTest2, lstmYTest2), \n",
    "                    epochs=6, \n",
    "                    batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
