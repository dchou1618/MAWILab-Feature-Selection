{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics:\n",
    "\n",
    "CPU \n",
    "Storage - A,B numbers are in storage\n",
    "Numbers read from storage into registers in cpu\n",
    "Then variables multiplied and stored in third register\n",
    "Third is written into storage\n",
    "\n",
    "If sequential:\n",
    "step latency - time it takes to finish 1 step\n",
    "total latency - time to finish all steps\n",
    "Main memory is fast\n",
    "spinning disk is slow\n",
    "\n",
    "Most latency is from memory latency, not operational latency\n",
    "Most latency is from reading and writing\n",
    "\n",
    "Sorting can improve memory locality\n",
    "Reason is money - local memory faster\n",
    "CPU checks if memory location in cache, and if it's not there,\n",
    "then retrieval will be slow. First case is a cache hit \n",
    "(when CPU intends to read a location and it's in cache)\n",
    "Second case is a cache miss.\n",
    "\n",
    "To handle a cache miss: \n",
    "(1) Free space in cache\n",
    "(2) After freeing, read in memory location and copy the block size\n",
    "Caches are effective if they have high hit rates.\n",
    "\n",
    "Unsorted word counts would entail temporal locality for common words, but\n",
    "no spatial locality.\n",
    "\n",
    "Caching reduces latency by bringing relevant data closer to the CPU.\n",
    "\n",
    "Access locality is the ability of software to make good use of the cache\n",
    "\n",
    "Temporal Locality = locality accessing the same elemnt over and over again\n",
    "\n",
    "Spatial Locality - ways to store n x terms to be squared. \n",
    "Linked lists have poor locality whereas indexed-arrays have\n",
    "good locality.\n",
    "\n",
    "Arrays store elements consecutively\n",
    "\n",
    "Row by row for array scanning is faster\n",
    "\n",
    "Effect increases proportionally with the number of elements in array\n",
    "\n",
    "\n",
    "Memory Hierarchy from small/fast storage closest to CPU at top.\n",
    "\n",
    "CPU (registers) - L2 Cache - Memory - Disk\n",
    "\n",
    "CPU registers -> L1/L2/L3 cache -> RAM -> SSDs -> Magnetic Tapes\n",
    "\n",
    "Data Centers are the physical aspect of \"the cloud\"\n",
    "\n",
    "\n",
    "HDFS - each file is broken into fixed-size chunks that are then copied.\n",
    "* Low cost per byte of storage \n",
    "* Locality\n",
    "* Redundancy - can recover from server failures\n",
    "* simple abstraction - looks like standard file system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@5649d52a\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3b61558f\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .appName(\"portdata-causality\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.retainGroupColumns\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "('pop from empty list',)",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "IndexError: pop from empty list"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "# import pandas as pd\n",
    "# probeDf = pd.read_csv(\"sparkProbeData.csv\")\n",
    "# sparkProbeDf = spark.createDataFrame(probeDf)\n",
    "# sparkProbeDf = sparkProbeDf.drop(\"Unnamed: 0\")\n",
    "# probeLabeledDf = pd.read_csv(\"sparkProbeLabeledData.csv\")\n",
    "# probeLabeledDf.head()\n",
    "# sparkProbeLabeledDf = spark.createDataFrame(probeLabeledDf)\n",
    "# sparkProbeLabeledDf = sparkProbeLabeledDf.drop(\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/* \n",
    "ip_flags, tcp_flags_ack, ip_dsfield, tcp_seq, tcp_flags_fin, tcp_flags_urg,tcp_flags_push\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "probeDf = spark.read.csv(\"newPortData.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "# \"ip_flags\", \"tcp_flags_ack\",\\\n",
    "#                 \"ip_dsfield\",\\\n",
    "#                 \"tcp_seq\", \"tcp_flags_fin\",\\\n",
    "#                 \"tcp_flags_urg\",\\\n",
    "#                 \"tcp_flags_push\",\\\n",
    "#                 \"tcp_options_mss_val\",\\\n",
    "#                 \"ip_ttl\",\\\n",
    "#                 \"tcp_window_size\",\\\n",
    "#                 \"tcp_checksum\",\\\n",
    "#                 \"tcp_srcport\",\\\n",
    "#                 \"tcp_dstport\",\\\n",
    "#                 \"label\"\n",
    "\n",
    "# \"frame_info_time\"\n",
    "# selected after feature importance with fake attributes\n",
    "probeDf = probeDf.select(*(\"frame_info_len\", \"tcp_ack\",\\\n",
    "                \"tcp_seq\",\\\n",
    "                \"ip_len\", \"tcp_flags\",\\\n",
    "                \"tcp_options_mss_val\",\\\n",
    "                \"ip_ttl\",\\\n",
    "                \"tcp_window_size\",\\\n",
    "                \"tcp_checksum\",\\\n",
    "                \"tcp_srcport\",\\\n",
    "                \"tcp_dstport\",\\\n",
    "                \"label\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "probeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- frame_info_len: integer (nullable = true)\n",
      " |-- tcp_ack: integer (nullable = true)\n",
      " |-- tcp_seq: integer (nullable = true)\n",
      " |-- ip_len: integer (nullable = true)\n",
      " |-- tcp_flags: integer (nullable = true)\n",
      " |-- tcp_options_mss_val: float (nullable = true)\n",
      " |-- ip_ttl: integer (nullable = true)\n",
      " |-- tcp_window_size: integer (nullable = true)\n",
      " |-- tcp_checksum: integer (nullable = true)\n",
      " |-- tcp_srcport: integer (nullable = true)\n",
      " |-- tcp_dstport: integer (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "from pyspark.sql.functions import col\n",
    "for col_ in probeDf.columns[:len(probeDf.columns)-1]:\n",
    "    if col_ == \"tcp_options_mss_val\":\n",
    "        probeDf = probeDf.withColumn(col_, col(col_).cast(\"float\"))\n",
    "    else:\n",
    "        probeDf = probeDf.withColumn(col_, col(col_).cast(\"integer\"))\n",
    "probeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(probeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"frame_info_len\", \"tcp_ack\",\\\n",
    "                \"tcp_seq\",\\\n",
    "                \"ip_len\", \"tcp_flags\",\\\n",
    "                \"tcp_options_mss_val\",\\\n",
    "                \"ip_ttl\",\\\n",
    "                \"tcp_window_size\",\\\n",
    "                \"tcp_checksum\",\\\n",
    "                \"tcp_srcport\",\\\n",
    "                \"tcp_dstport\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.setHandleInvalid(\"skip\").transform(probeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[551.0,1.0,1.0,53...|normal|\n",
      "|[94.0,1.0,1.0,80....|normal|\n",
      "|[68.0,1.0,1.0,54....|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[1440.0,1.0,1.0,1...|normal|\n",
      "|[1440.0,1.0,2749....|normal|\n",
      "|[1440.0,1.0,1.0,1...|normal|\n",
      "|[1440.0,1.0,31603...|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[1514.0,1.0,1.0,1...|normal|\n",
      "|[1514.0,1.0,1.0,1...|normal|\n",
      "|[1514.0,1.0,23714...|normal|\n",
      "|[1514.0,1.0,32474...|normal|\n",
      "|[2974.0,1.0,37207...|normal|\n",
      "|[1470.0,1.0,1.0,1...|normal|\n",
      "|[1514.0,1.0,94099...|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[1514.0,1.0,19403...|normal|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "probeDf = output.select(*(\"features\",\"label\"))\n",
    "probeDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\",\\\n",
    "    outputCol=\"indexedFeatures\",\\\n",
    "    maxCategories=3).fit(probeDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "DF = probeDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "labelTypes = ['normal', 'nmap_null', 'nmap_connect', 'zmap', 'nmap_window', 'masscan', 'hping_syn', 'unicorn_null', 'unicorn_syn', 'nmap_xmas', 'nmap_syn', 'unicorn_conn', 'unicorn_xmas', 'nmap_ack', 'hping_fin', 'nmap_maimon', 'hping_null', 'hping_xmas', 'hping_ack', 'nmap_fin', 'unicorn_fxmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"newPortData.csv\")\n",
    "\n",
    "indices = dict()\n",
    "for col in labelTypes:\n",
    "    indices[col] = list(df[df['label'] == col][\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "L = []\n",
    "L += [2]\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_: Any = [2]\n",
       "_20: Any = [2]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "/*!pip3 install imblearn*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[54.0, 0.0, 0.0, 40.0, 2.0, 17055.138671875, 2...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[551.0, 1.0, 1.0, 537.0, 24.0, 16923.056640625...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[94.0, 1.0, 1.0, 80.0, 16.0, 16738.109375, 59....</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[68.0, 1.0, 1.0, 54.0, 24.0, 16507.53515625, 5...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[54.0, 0.0, 0.0, 40.0, 2.0, 16237.595703125, 2...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features   label\n",
       "0  [54.0, 0.0, 0.0, 40.0, 2.0, 17055.138671875, 2...  normal\n",
       "1  [551.0, 1.0, 1.0, 537.0, 24.0, 16923.056640625...  normal\n",
       "2  [94.0, 1.0, 1.0, 80.0, 16.0, 16738.109375, 59....  normal\n",
       "3  [68.0, 1.0, 1.0, 54.0, 24.0, 16507.53515625, 5...  normal\n",
       "4  [54.0, 0.0, 0.0, 40.0, 2.0, 16237.595703125, 2...  normal"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%python\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|[94.0,1.0,1.0,80....|normal|\n",
      "|[68.0,1.0,1.0,54....|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[1440.0,1.0,1.0,1...|normal|\n",
      "|[1440.0,1.0,31603...|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[1514.0,1.0,1.0,1...|normal|\n",
      "|[1514.0,1.0,32474...|normal|\n",
      "|[2974.0,1.0,37207...|normal|\n",
      "|[1470.0,1.0,1.0,1...|normal|\n",
      "|[1514.0,1.0,94099...|normal|\n",
      "|[1514.0,1.0,19403...|normal|\n",
      "|[1514.0,1.0,31476...|normal|\n",
      "|[1514.0,1.0,32060...|normal|\n",
      "|[1514.0,1.0,32388...|normal|\n",
      "|[117.0,1.0,1.0,10...|normal|\n",
      "|[1514.0,1.0,17181...|normal|\n",
      "|[1440.0,1.0,11129...|normal|\n",
      "|[1514.0,1.0,49531...|normal|\n",
      "|[180.0,1.0,1.0,16...|normal|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "probeDf.sample(False, 0.5, seed=0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "# doing data split\n",
    "\n",
    "newTest = None\n",
    "newTrain = None\n",
    "\n",
    "for col in labelTypes:\n",
    "    allCol = probeDf.filter(probeDf[\"label\"]==col)\n",
    "    newColFiltered = allCol.sample(False, 0.5, seed=101)\n",
    "    if newTest is None:\n",
    "        newTest = (allCol).subtract(newColFiltered)\n",
    "    else:\n",
    "        newTest = newTest.union((allCol).subtract(newColFiltered))\n",
    "    \n",
    "    if newTrain is None:\n",
    "        newTrain = newColFiltered\n",
    "    else:\n",
    "        newTrain = newTrain.union(newColFiltered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "#(trainingData, testData) = probeDf.randomSplit([0.70, 0.30])\n",
    "\n",
    "rfProbe = RandomForestClassifier(labelCol=\"indexedLabel\", \n",
    "                                 featuresCol=\"indexedFeatures\", \n",
    "                                 numTrees = 5,\n",
    "                                 maxDepth = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rfProbe])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[94.0,1.0,1.0,80....|normal|\n",
      "|[1440.0,1.0,1.0,1...|normal|\n",
      "|[1440.0,1.0,2749....|normal|\n",
      "|[1440.0,1.0,1.0,1...|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[1514.0,1.0,1.0,1...|normal|\n",
      "|[1470.0,1.0,1.0,1...|normal|\n",
      "|[1514.0,1.0,28048...|normal|\n",
      "|[1514.0,1.0,32388...|normal|\n",
      "|[117.0,1.0,1.0,10...|normal|\n",
      "|[1440.0,1.0,70075...|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[1440.0,1.0,93433...|normal|\n",
      "|[54.0,0.0,0.0,40....|normal|\n",
      "|[1514.0,1.0,17181...|normal|\n",
      "|[1440.0,1.0,11129...|normal|\n",
      "|[1514.0,1.0,21368...|normal|\n",
      "|[180.0,1.0,1.0,16...|normal|\n",
      "|[1514.0,1.0,22722...|normal|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "newTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "model = pipeline.fit(newTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "predictions = model.transform(newTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,[0,1,2,3,4,5,6,7,8,9,10],[0.10225270767747473,0.04275038269299825,0.028624979124243746,0.014479264860995038,0.08524591797888197,0.02793738248619991,0.3552375167831205,0.12485313758240703,0.003330396690295598,0.059554574006360626,0.15573374011702254])\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel.featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
